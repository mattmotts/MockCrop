{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the annotations file\n",
    "annotations_file = 'data/training_annotations.csv'\n",
    "annotations_df = pd.read_csv(annotations_file)\n",
    "\n",
    "# Function to check if image paths are correct and display images\n",
    "def check_image_paths(df, image_column):\n",
    "    for index, row in df.iterrows():\n",
    "        image_path = row[image_column]\n",
    "        try:\n",
    "            # Open the image to check if the path is correct\n",
    "            image = Image.open(image_path)\n",
    "            # Display the image\n",
    "            plt.imshow(image)\n",
    "            plt.title(f\"Image ID: {row['image_id']}\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "\n",
    "# Run the check\n",
    "check_image_paths(annotations_df, 'image_path')\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotations_df, transform=None):\n",
    "        self.annotations = annotations_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations.iloc[idx]\n",
    "        image_path = annotation['image_path']\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Parse bounding boxes\n",
    "        boxes = [[annotation['left'], annotation['top'],\n",
    "                  annotation['left'] + annotation['width'], annotation['top'] + annotation['height']]]\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)  # Ensure the shape is [N, 4]\n",
    "\n",
    "        # Parse labels\n",
    "        labels = torch.tensor([1], dtype=torch.int64)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        return image, target\n",
    "    \n",
    "    \n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = CustomDataset(annotations_df, transform=transform)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Ensure at least one sample in each set\n",
    "if val_size == 0:\n",
    "    val_size = 1\n",
    "    train_size -= 1\n",
    "if test_size == 0:\n",
    "    test_size = 1\n",
    "    train_size -= 1\n",
    "\n",
    "print(f'Train size: {train_size}, Validation size: {val_size}, Test size: {test_size}')\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Data loaders with num_workers set to 0\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "# Define model\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 2  # 1 class (frame) + background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # # Check the structure of images and targets\n",
    "        # print(f'Images: {type(images)}, Targets: {type(targets)}')\n",
    "        # print(f'Images shape: {images.shape}')\n",
    "        \n",
    "        # # Check the keys in targets\n",
    "        # print(f'Targets keys: {targets.keys()}')\n",
    "        # # Check the first target's structure\n",
    "        # print(f'First target structure: {targets}')\n",
    "        \n",
    "        # Convert targets to list of dictionaries\n",
    "        targets_list = []\n",
    "        for i in range(images.size(0)):\n",
    "            target_dict = {}\n",
    "            target_dict['boxes'] = targets['boxes'][i]\n",
    "            target_dict['labels'] = targets['labels'][i]\n",
    "            targets_list.append(target_dict)\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets_list)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        epoch_loss += losses.item()\n",
    "        # Backward pass\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "    # # Validation loop\n",
    "    # model.eval()\n",
    "    # val_loss = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for images, targets in val_loader:\n",
    "    #         # Convert targets to list of dictionaries\n",
    "    #         targets_list = []\n",
    "    #         for i in range(images.size(0)):\n",
    "    #             target_dict = {}\n",
    "    #             target_dict['boxes'] = targets['boxes'][i]\n",
    "    #             target_dict['labels'] = targets['labels'][i]\n",
    "    #             targets_list.append(target_dict)\n",
    "\n",
    "    #         # Forward pass\n",
    "    #         outputs = model(images, targets_list)\n",
    "\n",
    "    #         # Calculate validation loss if in training mode\n",
    "    #         if model.training:\n",
    "    #             loss_dict = outputs\n",
    "    #             losses = sum(loss for loss in loss_dict.values())\n",
    "    #             val_loss += losses.item()\n",
    "    #         else:\n",
    "    #             val_loss += 0  # No loss calculation in eval mode\n",
    "\n",
    "    # if len(val_loader) > 0:\n",
    "    #     print(f\"Epoch {epoch+1}, Validation Loss: {val_loss / len(val_loader)}\")\n",
    "    # else:\n",
    "    #     print(f\"Epoch {epoch+1}, Validation Loss: No validation samples\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# Load the model for inference\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell by Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotations file\n",
    "annotations_file = 'data/training_annotations.csv'\n",
    "annotations_df = pd.read_csv(annotations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if image paths are correct and display images\n",
    "def check_image_paths(df, image_column):\n",
    "    for index, row in df.iterrows():\n",
    "        image_path = row[image_column]\n",
    "        try:\n",
    "            # Open the image to check if the path is correct\n",
    "            image = Image.open(image_path)\n",
    "            # Display the image\n",
    "            plt.imshow(image)\n",
    "            plt.title(f\"Image ID: {row['image_id']}\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "\n",
    "# Run the check\n",
    "check_image_paths(annotations_df, 'image_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotations_df, transform=None):\n",
    "        self.annotations = annotations_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations.iloc[idx]\n",
    "        image_path = annotation['image_path']\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Parse bounding boxes\n",
    "        boxes = [[annotation['left'], annotation['top'],\n",
    "                  annotation['left'] + annotation['width'], annotation['top'] + annotation['height']]]\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)  # Ensure the shape is [N, 4]\n",
    "\n",
    "        # Parse labels\n",
    "        labels = torch.tensor([1], dtype=torch.int64)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset\n",
    "dataset = CustomDataset(annotations_df, transform=transform)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Ensure at least one sample in each set\n",
    "if val_size == 0:\n",
    "    val_size = 1\n",
    "    train_size -= 1\n",
    "if test_size == 0:\n",
    "    test_size = 1\n",
    "    train_size -= 1\n",
    "\n",
    "print(f'Train size: {train_size}, Validation size: {val_size}, Test size: {test_size}')\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Data loaders with num_workers set to 0\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 2  # 1 class (frame) + background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # # Check the structure of images and targets\n",
    "        # print(f'Images: {type(images)}, Targets: {type(targets)}')\n",
    "        # print(f'Images shape: {images.shape}')\n",
    "        \n",
    "        # # Check the keys in targets\n",
    "        # print(f'Targets keys: {targets.keys()}')\n",
    "        # # Check the first target's structure\n",
    "        # print(f'First target structure: {targets}')\n",
    "        \n",
    "        # Convert targets to list of dictionaries\n",
    "        targets_list = []\n",
    "        for i in range(images.size(0)):\n",
    "            target_dict = {}\n",
    "            target_dict['boxes'] = targets['boxes'][i]\n",
    "            target_dict['labels'] = targets['labels'][i]\n",
    "            targets_list.append(target_dict)\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets_list)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        epoch_loss += losses.item()\n",
    "        # Backward pass\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            # Convert targets to list of dictionaries\n",
    "            targets_list = []\n",
    "            for i in range(images.size(0)):\n",
    "                target_dict = {}\n",
    "                target_dict['boxes'] = targets['boxes'][i]\n",
    "                target_dict['labels'] = targets['labels'][i]\n",
    "                targets_list.append(target_dict)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, targets_list)\n",
    "\n",
    "            # Calculate validation loss if in training mode\n",
    "            if model.training:\n",
    "                loss_dict = outputs\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                val_loss += losses.item()\n",
    "            else:\n",
    "                val_loss += 0  # No loss calculation in eval mode\n",
    "\n",
    "    if len(val_loader) > 0:\n",
    "        print(f\"Epoch {epoch+1}, Validation Loss: {val_loss / len(val_loader)}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}, Validation Loss: No validation samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# Load the model for inference\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "1. Prepare the test data\n",
    "2. Load the trained model\n",
    "3. Run inference on test images \n",
    "4. Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Function to run inference on a single image\n",
    "def run_inference(model, image_path, transform):\n",
    "    # Load and transform the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Move the image to the same device as the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Run inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(image_tensor)[0]\n",
    "\n",
    "    return prediction\n",
    "\n",
    "# Directory containing test images\n",
    "test_image_dir = 'images/Testing'\n",
    "\n",
    "# Transform for the images\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    # Add other transformations if needed\n",
    "])\n",
    "\n",
    "# Load the model\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "# Get list of test image paths\n",
    "test_image_paths = [os.path.join(test_image_dir, fname) for fname in os.listdir(test_image_dir) if fname.endswith('.png')]\n",
    "\n",
    "# Run inference on all test images\n",
    "for image_path in test_image_paths:\n",
    "    prediction = run_inference(model, image_path, transform)\n",
    "    \n",
    "    # Display the image and the predicted bounding boxes\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    for box, label, score in zip(prediction['boxes'], prediction['labels'], prediction['scores']):\n",
    "        if score > 0.5:  # Confidence threshold\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color='red')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(xmin, ymin, f'{label.item()}:{score:.2f}', bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    plt.title(f\"Predictions for {os.path.basename(image_path)}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Function to load annotations from CSV\n",
    "def load_annotations(csv_file):\n",
    "    return pd.read_csv(csv_file)\n",
    "\n",
    "# Load the ground truth annotations for test images\n",
    "test_annotations_file = 'data/testing_annotations.csv'\n",
    "test_annotations_df = load_annotations(test_annotations_file)\n",
    "\n",
    "# Define the IoU calculation function\n",
    "def calculate_iou(boxA, boxB):\n",
    "    # Determine the coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    # Compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yA - yB + 1)\n",
    "    # Compute the area of both the prediction and ground-truth rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    # Compute the intersection over union\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "# Run inference on a single image\n",
    "def run_inference(model, image_path):\n",
    "    # Load and transform the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "    \n",
    "    # Run the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(image_tensor)[0]\n",
    "    \n",
    "    # Extract the boxes and labels\n",
    "    boxes = prediction['boxes'].cpu().numpy()\n",
    "    labels = prediction['labels'].cpu().numpy()\n",
    "    scores = prediction['scores'].cpu().numpy()\n",
    "    \n",
    "    return image, boxes, labels, scores\n",
    "\n",
    "# Evaluate the model on test images\n",
    "def evaluate_model(model, test_annotations_df):\n",
    "    for index, row in test_annotations_df.iterrows():\n",
    "        image_path = row['image_path']\n",
    "        ground_truth_box = [row['left'], row['top'], row['left'] + row['width'], row['top'] + row['height']]\n",
    "        \n",
    "        # Run inference\n",
    "        image, predicted_boxes, labels, scores = run_inference(model, image_path)\n",
    "        \n",
    "        # Display the image and the predicted bounding boxes\n",
    "        plt.imshow(image)\n",
    "        ax = plt.gca()\n",
    "        for box, label, score in zip(predicted_boxes, labels, scores):\n",
    "            if score > 0.5:  # Confidence threshold\n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color='red')\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(xmin, ymin, f'{label.item()}:{score:.2f}', bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "        \n",
    "        # Draw the ground truth box in green\n",
    "        xmin, ymin, xmax, ymax = ground_truth_box\n",
    "        rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color='green')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        plt.title(f\"Predictions for {os.path.basename(image_path)}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate IoU\n",
    "        if len(predicted_boxes) > 0:\n",
    "            iou = calculate_iou(predicted_boxes[0], ground_truth_box)\n",
    "            print(f\"IoU for {os.path.basename(image_path)}: {iou:.2f}\")\n",
    "\n",
    "# Load the model\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 2  # 1 class (frame) + background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "# Evaluate the model on test images\n",
    "evaluate_model(model, test_annotations_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotations file\n",
    "annotations_file = 'data/training_annotations.csv'\n",
    "annotations_df = pd.read_csv(annotations_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_image_paths(df, image_column):\n",
    "#     for index, row in df.iterrows():\n",
    "#         image_path = row[image_column]\n",
    "#         try:\n",
    "#             # Open the image to check if the path is correct\n",
    "#             image = Image.open(image_path)\n",
    "#             # Display the image\n",
    "#             plt.imshow(image)\n",
    "#             plt.title(f\"Image ID: {row['image_id']}\")\n",
    "#             plt.show()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading image {image_path}: {e}\")\n",
    "\n",
    "# # Run the check\n",
    "# check_image_paths(annotations_df, 'image_path')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotations_df, transform=None):\n",
    "        self.annotations = annotations_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations.iloc[idx]\n",
    "        image_path = annotation['image_path']\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Parse bounding boxes\n",
    "        boxes = [[annotation['left'], annotation['top'],\n",
    "                  annotation['left'] + annotation['width'], annotation['top'] + annotation['height']]]\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)  # Ensure the shape is [N, 4]\n",
    "\n",
    "        # Parse labels\n",
    "        labels = torch.tensor([1], dtype=torch.int64)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why am I doing a train/validation and test sample? Why can't I just do a train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset\n",
    "dataset = CustomDataset(annotations_df, transform=transform)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Ensure at least one sample in each set\n",
    "if val_size == 0:\n",
    "    val_size = 1\n",
    "    train_size -= 1\n",
    "if test_size == 0:\n",
    "    test_size = 1\n",
    "    train_size -= 1\n",
    "\n",
    "print(f'Train size: {train_size}, Validation size: {val_size}, Test size: {test_size}')\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Data loaders with num_workers set to 0\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 2  # 1 class (frame) + background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Convert targets to list of dictionaries\n",
    "        targets_list = []\n",
    "        for i in range(len(targets)):\n",
    "            target_dict = {}\n",
    "            target_dict['boxes'] = targets[i]['boxes']\n",
    "            target_dict['labels'] = targets[i]['labels']\n",
    "            targets_list.append(target_dict)\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets_list)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        epoch_loss += losses.item()\n",
    "        # Backward pass\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {epoch_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation loop\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for images, targets in val_loader:\n",
    "        # Convert targets to list of dictionaries\n",
    "        targets_list = []\n",
    "        for i in range(len(targets)):\n",
    "            target_dict = {}\n",
    "            target_dict['boxes'] = targets[i]['boxes']\n",
    "            target_dict['labels'] = targets[i]['labels']\n",
    "            targets_list.append(target_dict)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, targets_list)\n",
    "        loss_dict = model(images, targets_list)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        val_loss += losses.item()\n",
    "\n",
    "if len(val_loader) > 0:\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss / len(val_loader)}\")\n",
    "else:\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: No validation samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# Load the model for inference\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_frames(model, image, threshold=0.5):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    image_tensor = transform(image).unsqueeze(0)  # Convert image to tensor and add batch dimension\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "    \n",
    "    # Filter out detections with low scores\n",
    "    boxes = outputs[0]['boxes'][outputs[0]['scores'] > threshold]\n",
    "    return boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw Bounding Boxes and Overlay Graphical Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def overlay_graphical_image(mockup_image, boxes, graphical_image):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes on the mockup image and overlays a graphical image onto the detected frames.\n",
    "    \n",
    "    Parameters:\n",
    "    - mockup_image: PIL Image object, the mockup image where frames are detected.\n",
    "    - boxes: Tensor, bounding boxes detected by the model.\n",
    "    - graphical_image: PIL Image object, the graphical image that will be overlayed.\n",
    "    \n",
    "    Returns:\n",
    "    - mockup_image: PIL Image object, the mockup image with bounding boxes and overlayed graphical image.\n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(mockup_image)\n",
    "    \n",
    "    for box in boxes:\n",
    "        left, top, right, bottom = box\n",
    "        \n",
    "        # Draw bounding box\n",
    "        draw.rectangle([(left, top), (right, bottom)], outline=\"green\", width=3)\n",
    "        \n",
    "        # Resize graphical image to fit the bounding box\n",
    "        graphical_resized = graphical_image.resize((int(right - left), int(bottom - top)))\n",
    "        \n",
    "        # Paste graphical image onto the original image (mockup image)\n",
    "        mockup_image.paste(graphical_resized, (int(left), int(top)), graphical_resized)\n",
    "    \n",
    "    return mockup_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the single graphical image\n",
    "graphical_image_path = 'data/graphical_image.png'  # Replace with your graphical image path\n",
    "graphical_image = Image.open(graphical_image_path).convert(\"RGBA\")\n",
    "\n",
    "# Directory containing mockup images to process\n",
    "mockup_images_dir = 'data/mockup_images'  # Replace with your directory path\n",
    "\n",
    "# Loop through each mockup image in the directory and apply overlay\n",
    "for mockup_image_file in os.listdir(mockup_images_dir):\n",
    "    mockup_image_path = os.path.join(mockup_images_dir, mockup_image_file)\n",
    "    \n",
    "    # Load the mockup image\n",
    "    mockup_image = Image.open(mockup_image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Perform detection\n",
    "    detected_boxes = detect_frames(model, mockup_image)\n",
    "    \n",
    "    # Overlay the graphical image and draw bounding boxes\n",
    "    result_image = overlay_graphical_image(mockup_image.copy(), detected_boxes, graphical_image)\n",
    "    \n",
    "    # Display the result\n",
    "    plt.imshow(result_image)\n",
    "    plt.title(f\"Detected Frames with Overlay: {mockup_image_file}\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
